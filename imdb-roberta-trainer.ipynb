{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport logging\nimport datasets\n\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification, DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-13T12:01:22.380586Z","iopub.execute_input":"2023-01-13T12:01:22.381111Z","iopub.status.idle":"2023-01-13T12:01:30.750121Z","shell.execute_reply.started":"2023-01-13T12:01:22.380994Z","shell.execute_reply":"2023-01-13T12:01:30.749046Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:30.753939Z","iopub.execute_input":"2023-01-13T12:01:30.754544Z","iopub.status.idle":"2023-01-13T12:01:32.346627Z","shell.execute_reply.started":"2023-01-13T12:01:30.754505Z","shell.execute_reply":"2023-01-13T12:01:32.345632Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"program = os.path.basename(sys.argv[0])\nlogger = logging.getLogger(program)\n\nlogging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\nlogging.root.setLevel(level=logging.INFO)\nlogger.info(r\"running %s\" % ''.join(sys.argv))\n\ntrain, val = train_test_split(train, test_size=.2)\n\ntrain_dict = {'label': train[\"sentiment\"], 'text': train['review']}\nval_dict = {'label': val[\"sentiment\"], 'text': val['review']}\ntest_dict = {\"text\": test['review']}\n\ntrain_dataset = datasets.Dataset.from_dict(train_dict)\nval_dataset = datasets.Dataset.from_dict(val_dict)\ntest_dataset = datasets.Dataset.from_dict(test_dict)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:32.348100Z","iopub.execute_input":"2023-01-13T12:01:32.348774Z","iopub.status.idle":"2023-01-13T12:01:32.603181Z","shell.execute_reply.started":"2023-01-13T12:01:32.348737Z","shell.execute_reply":"2023-01-13T12:01:32.602143Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:32.607832Z","iopub.execute_input":"2023-01-13T12:01:32.611244Z","iopub.status.idle":"2023-01-13T12:01:39.833175Z","shell.execute_reply.started":"2023-01-13T12:01:32.611202Z","shell.execute_reply":"2023-01-13T12:01:39.832218Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc08a53fe87428eb2efff12148597ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7594c4c22fe4ff6bbf73cd92b47892e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba2dd22b4924d129ec17e256ab23892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00511b636054cb7a8cfcb03a8ca0578"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples['text'], truncation=True)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:39.834542Z","iopub.execute_input":"2023-01-13T12:01:39.834992Z","iopub.status.idle":"2023-01-13T12:02:16.866605Z","shell.execute_reply.started":"2023-01-13T12:01:39.834956Z","shell.execute_reply":"2023-01-13T12:02:16.865377Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531f77c6e56f42c7bcf0470a15682b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf6058f68519472a9c1a679472576150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269aa7f03dbd4deeb32ad65bd81335ec"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:16.868568Z","iopub.execute_input":"2023-01-13T12:02:16.869232Z","iopub.status.idle":"2023-01-13T12:02:16.874481Z","shell.execute_reply.started":"2023-01-13T12:02:16.869193Z","shell.execute_reply":"2023-01-13T12:02:16.873150Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:16.875792Z","iopub.execute_input":"2023-01-13T12:02:16.876708Z","iopub.status.idle":"2023-01-13T12:02:28.566375Z","shell.execute_reply.started":"2023-01-13T12:02:16.876672Z","shell.execute_reply":"2023-01-13T12:02:28.565457Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610ae0928bd54320ad3dce57073353d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"metric = datasets.load_metric(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:28.567724Z","iopub.execute_input":"2023-01-13T12:02:28.568165Z","iopub.status.idle":"2023-01-13T12:02:29.576473Z","shell.execute_reply.started":"2023-01-13T12:02:28.568128Z","shell.execute_reply":"2023-01-13T12:02:29.575535Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"626e641ff79e41729efe42d5b14d4977"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ntraining_args = TrainingArguments(\n    output_dir='./results',  # output directory\n    num_train_epochs=3,  # total number of training epochs\n    per_device_train_batch_size=4,  # batch size per device during training\n    per_device_eval_batch_size=8,  # batch size for evaluation\n    learning_rate=5e-6,\n    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,  # strength of weight decay\n    logging_dir='./logs',  # directory for storing logs\n    logging_steps=100,\n    save_strategy=\"no\",\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,  # the instantiated  Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=tokenized_train,  # training dataset\n    eval_dataset=tokenized_val,  # evaluation dataset\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:29.577919Z","iopub.execute_input":"2023-01-13T12:02:29.578943Z","iopub.status.idle":"2023-01-13T12:02:34.380184Z","shell.execute_reply.started":"2023-01-13T12:02:29.578904Z","shell.execute_reply":"2023-01-13T12:02:34.379126Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:34.386608Z","iopub.execute_input":"2023-01-13T12:02:34.389273Z","iopub.status.idle":"2023-01-13T13:01:20.980449Z","shell.execute_reply.started":"2023-01-13T12:02:34.389232Z","shell.execute_reply":"2023-01-13T13:01:20.979529Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 20000\n  Num Epochs = 3\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 15000\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230113_120244-33yuvmrf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/yn/huggingface/runs/33yuvmrf\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/yn/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15000/15000 58:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.322100</td>\n      <td>0.280296</td>\n      <td>0.939200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.154700</td>\n      <td>0.343590</td>\n      <td>0.942800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.169100</td>\n      <td>0.339339</td>\n      <td>0.947400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 8\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15000, training_loss=0.24587234342892964, metrics={'train_runtime': 3526.5244, 'train_samples_per_second': 17.014, 'train_steps_per_second': 4.253, 'total_flos': 1.295426047731888e+16, 'train_loss': 0.24587234342892964, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"prediction_outputs = trainer.predict(tokenized_test)\ntest_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\nprint(test_pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:01:20.981908Z","iopub.execute_input":"2023-01-13T13:01:20.983640Z","iopub.status.idle":"2023-01-13T13:08:22.940949Z","shell.execute_reply.started":"2023-01-13T13:01:20.983601Z","shell.execute_reply":"2023-01-13T13:08:22.939960Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 25000\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3125/3125 07:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[1 0 0 ... 0 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\nresult_output.to_csv(\"/kaggle/working/imdb_roberta_trainer.csv\", index=False, quoting=3)\nlogging.info('result saved!')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:08:22.942346Z","iopub.execute_input":"2023-01-13T13:08:22.942992Z","iopub.status.idle":"2023-01-13T13:08:22.969106Z","shell.execute_reply.started":"2023-01-13T13:08:22.942950Z","shell.execute_reply":"2023-01-13T13:08:22.968232Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}