{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport logging\nimport datasets\n\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification, DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-13T12:01:22.380586Z","iopub.execute_input":"2023-01-13T12:01:22.381111Z","iopub.status.idle":"2023-01-13T12:01:30.750121Z","shell.execute_reply.started":"2023-01-13T12:01:22.380994Z","shell.execute_reply":"2023-01-13T12:01:30.749046Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:30.753939Z","iopub.execute_input":"2023-01-13T12:01:30.754544Z","iopub.status.idle":"2023-01-13T12:01:32.346627Z","shell.execute_reply.started":"2023-01-13T12:01:30.754505Z","shell.execute_reply":"2023-01-13T12:01:32.345632Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"program = os.path.basename(sys.argv[0])\nlogger = logging.getLogger(program)\n\nlogging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\nlogging.root.setLevel(level=logging.INFO)\nlogger.info(r\"running %s\" % ''.join(sys.argv))\n\ntrain, val = train_test_split(train, test_size=.2)\n\ntrain_dict = {'label': train[\"sentiment\"], 'text': train['review']}\nval_dict = {'label': val[\"sentiment\"], 'text': val['review']}\ntest_dict = {\"text\": test['review']}\n\ntrain_dataset = datasets.Dataset.from_dict(train_dict)\nval_dataset = datasets.Dataset.from_dict(val_dict)\ntest_dataset = datasets.Dataset.from_dict(test_dict)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:32.348100Z","iopub.execute_input":"2023-01-13T12:01:32.348774Z","iopub.status.idle":"2023-01-13T12:01:32.603181Z","shell.execute_reply.started":"2023-01-13T12:01:32.348737Z","shell.execute_reply":"2023-01-13T12:01:32.602143Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:32.607832Z","iopub.execute_input":"2023-01-13T12:01:32.611244Z","iopub.status.idle":"2023-01-13T12:01:39.833175Z","shell.execute_reply.started":"2023-01-13T12:01:32.611202Z","shell.execute_reply":"2023-01-13T12:01:39.832218Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc08a53fe87428eb2efff12148597ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7594c4c22fe4ff6bbf73cd92b47892e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba2dd22b4924d129ec17e256ab23892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00511b636054cb7a8cfcb03a8ca0578"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples['text'], truncation=True)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:01:39.834542Z","iopub.execute_input":"2023-01-13T12:01:39.834992Z","iopub.status.idle":"2023-01-13T12:02:16.866605Z","shell.execute_reply.started":"2023-01-13T12:01:39.834956Z","shell.execute_reply":"2023-01-13T12:02:16.865377Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531f77c6e56f42c7bcf0470a15682b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf6058f68519472a9c1a679472576150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269aa7f03dbd4deeb32ad65bd81335ec"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:16.868568Z","iopub.execute_input":"2023-01-13T12:02:16.869232Z","iopub.status.idle":"2023-01-13T12:02:16.874481Z","shell.execute_reply.started":"2023-01-13T12:02:16.869193Z","shell.execute_reply":"2023-01-13T12:02:16.873150Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:16.875792Z","iopub.execute_input":"2023-01-13T12:02:16.876708Z","iopub.status.idle":"2023-01-13T12:02:28.566375Z","shell.execute_reply.started":"2023-01-13T12:02:16.876672Z","shell.execute_reply":"2023-01-13T12:02:28.565457Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610ae0928bd54320ad3dce57073353d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"metric = datasets.load_metric(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:28.567724Z","iopub.execute_input":"2023-01-13T12:02:28.568165Z","iopub.status.idle":"2023-01-13T12:02:29.576473Z","shell.execute_reply.started":"2023-01-13T12:02:28.568128Z","shell.execute_reply":"2023-01-13T12:02:29.575535Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"626e641ff79e41729efe42d5b14d4977"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ntraining_args = TrainingArguments(\n    output_dir='./results',  # output directory\n    num_train_epochs=3,  # total number of training epochs\n    per_device_train_batch_size=4,  # batch size per device during training\n    per_device_eval_batch_size=8,  # batch size for evaluation\n    learning_rate=5e-6,\n    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,  # strength of weight decay\n    logging_dir='./logs',  # directory for storing logs\n    logging_steps=100,\n    save_strategy=\"no\",\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=tokenized_train,  # training dataset\n    eval_dataset=tokenized_val,  # evaluation dataset\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:29.577919Z","iopub.execute_input":"2023-01-13T12:02:29.578943Z","iopub.status.idle":"2023-01-13T12:02:34.380184Z","shell.execute_reply.started":"2023-01-13T12:02:29.578904Z","shell.execute_reply":"2023-01-13T12:02:34.379126Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T12:02:34.386608Z","iopub.execute_input":"2023-01-13T12:02:34.389273Z","iopub.status.idle":"2023-01-13T13:01:20.980449Z","shell.execute_reply.started":"2023-01-13T12:02:34.389232Z","shell.execute_reply":"2023-01-13T13:01:20.979529Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 20000\n  Num Epochs = 3\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 15000\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230113_120244-33yuvmrf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/yn/huggingface/runs/33yuvmrf\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/yn/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15000/15000 58:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.322100</td>\n      <td>0.280296</td>\n      <td>0.939200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.154700</td>\n      <td>0.343590</td>\n      <td>0.942800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.169100</td>\n      <td>0.339339</td>\n      <td>0.947400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 8\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15000, training_loss=0.24587234342892964, metrics={'train_runtime': 3526.5244, 'train_samples_per_second': 17.014, 'train_steps_per_second': 4.253, 'total_flos': 1.295426047731888e+16, 'train_loss': 0.24587234342892964, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"prediction_outputs = trainer.predict(tokenized_test)\ntest_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\nprint(test_pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:01:20.981908Z","iopub.execute_input":"2023-01-13T13:01:20.983640Z","iopub.status.idle":"2023-01-13T13:08:22.940949Z","shell.execute_reply.started":"2023-01-13T13:01:20.983601Z","shell.execute_reply":"2023-01-13T13:08:22.939960Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 25000\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3125/3125 07:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[1 0 0 ... 0 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\nresult_output.to_csv(\"/kaggle/working/imdb_roberta_trainer.csv\", index=False, quoting=3)\nlogging.info('result saved!')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:08:22.942346Z","iopub.execute_input":"2023-01-13T13:08:22.942992Z","iopub.status.idle":"2023-01-13T13:08:22.969106Z","shell.execute_reply.started":"2023-01-13T13:08:22.942950Z","shell.execute_reply":"2023-01-13T13:08:22.968232Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}