{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport logging\nimport datasets\n\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-13T11:16:28.358188Z","iopub.execute_input":"2023-01-13T11:16:28.359016Z","iopub.status.idle":"2023-01-13T11:16:36.989878Z","shell.execute_reply.started":"2023-01-13T11:16:28.358918Z","shell.execute_reply":"2023-01-13T11:16:36.988869Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:16:36.992243Z","iopub.execute_input":"2023-01-13T11:16:36.993295Z","iopub.status.idle":"2023-01-13T11:16:38.353999Z","shell.execute_reply.started":"2023-01-13T11:16:36.993251Z","shell.execute_reply":"2023-01-13T11:16:38.353056Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"program = os.path.basename(sys.argv[0])\nlogger = logging.getLogger(program)\n\nlogging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\nlogging.root.setLevel(level=logging.INFO)\nlogger.info(r\"running %s\" % ''.join(sys.argv))\n\ntrain, val = train_test_split(train, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:16:38.355315Z","iopub.execute_input":"2023-01-13T11:16:38.356714Z","iopub.status.idle":"2023-01-13T11:16:38.372756Z","shell.execute_reply.started":"2023-01-13T11:16:38.356675Z","shell.execute_reply":"2023-01-13T11:16:38.371764Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\nval_dict = {'label': val[\"sentiment\"], 'text': val['review']}\ntest_dict = {\"text\": test['review']}\n\ntrain_dataset = datasets.Dataset.from_dict(train_dict)\nval_dataset = datasets.Dataset.from_dict(val_dict)\ntest_dataset = datasets.Dataset.from_dict(test_dict)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:16:38.376468Z","iopub.execute_input":"2023-01-13T11:16:38.377031Z","iopub.status.idle":"2023-01-13T11:16:38.585196Z","shell.execute_reply.started":"2023-01-13T11:16:38.376997Z","shell.execute_reply":"2023-01-13T11:16:38.584177Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:16:38.586533Z","iopub.execute_input":"2023-01-13T11:16:38.587284Z","iopub.status.idle":"2023-01-13T11:16:42.956451Z","shell.execute_reply.started":"2023-01-13T11:16:38.587246Z","shell.execute_reply":"2023-01-13T11:16:42.955563Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99796342ed9424b9bcfc414557b0083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71728f58764c4a82aafb29511834ac0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a02d0d194db4b6d8da932af4a4cde6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ddc231930f2450e80a5b4d95903277b"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples['text'], truncation=True)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:16:42.957969Z","iopub.execute_input":"2023-01-13T11:16:42.958367Z","iopub.status.idle":"2023-01-13T11:17:18.316082Z","shell.execute_reply.started":"2023-01-13T11:16:42.958310Z","shell.execute_reply":"2023-01-13T11:17:18.314898Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d31264763434248808bb0615785765c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb69d0926fd4b7b94b520ced7962036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9210387b045c4575b10c01781b9418bd"}},"metadata":{}}]},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:17:18.317681Z","iopub.execute_input":"2023-01-13T11:17:18.318075Z","iopub.status.idle":"2023-01-13T11:17:32.441070Z","shell.execute_reply.started":"2023-01-13T11:17:18.318039Z","shell.execute_reply":"2023-01-13T11:17:32.439918Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240484fe456e4dbabc3121527cf4da14"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"metric = datasets.load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:17:32.442596Z","iopub.execute_input":"2023-01-13T11:17:32.443307Z","iopub.status.idle":"2023-01-13T11:17:33.235695Z","shell.execute_reply.started":"2023-01-13T11:17:32.443265Z","shell.execute_reply":"2023-01-13T11:17:33.234723Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eed0f9ef8ff45449e4f20193c58eb9d"}},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',  # output directory\n    num_train_epochs=3,  # total number of training epochs\n    per_device_train_batch_size=12,  # batch size per device during training\n    per_device_eval_batch_size=24,  # batch size for evaluation\n    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,  # strength of weight decay\n    logging_dir='./logs',  # directory for storing logs\n    logging_steps=100,\n    save_strategy=\"no\",\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=tokenized_train,  # training dataset\n    eval_dataset=tokenized_val,  # evaluation dataset\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:17:33.237471Z","iopub.execute_input":"2023-01-13T11:17:33.237832Z","iopub.status.idle":"2023-01-13T11:17:37.861851Z","shell.execute_reply.started":"2023-01-13T11:17:33.237796Z","shell.execute_reply":"2023-01-13T11:17:37.860855Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:17:37.863516Z","iopub.execute_input":"2023-01-13T11:17:37.863860Z","iopub.status.idle":"2023-01-13T11:48:07.360081Z","shell.execute_reply.started":"2023-01-13T11:17:37.863824Z","shell.execute_reply":"2023-01-13T11:48:07.359163Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 20000\n  Num Epochs = 3\n  Instantaneous batch size per device = 12\n  Total train batch size (w. parallel, distributed & accumulation) = 12\n  Gradient Accumulation steps = 1\n  Total optimization steps = 5001\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230113_111825-vpn9om4c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/yn/huggingface/runs/vpn9om4c\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/yn/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5001' max='5001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5001/5001 29:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.257300</td>\n      <td>0.237643</td>\n      <td>0.914000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.155800</td>\n      <td>0.274278</td>\n      <td>0.925000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.062700</td>\n      <td>0.362830</td>\n      <td>0.924000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 24\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 24\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 24\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5001, training_loss=0.18610320106566614, metrics={'train_runtime': 1829.4484, 'train_samples_per_second': 32.797, 'train_steps_per_second': 2.734, 'total_flos': 7792746592466880.0, 'train_loss': 0.18610320106566614, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"prediction_outputs = trainer.predict(tokenized_test)\ntest_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\nprint(test_pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:48:07.366307Z","iopub.execute_input":"2023-01-13T11:48:07.368432Z","iopub.status.idle":"2023-01-13T11:51:45.383319Z","shell.execute_reply.started":"2023-01-13T11:48:07.368393Z","shell.execute_reply":"2023-01-13T11:51:45.382409Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 25000\n  Batch size = 24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1042' max='1042' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1042/1042 03:37]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[1 0 0 ... 0 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\nresult_output.to_csv(\"/kaggle/working/imdb_distilbert_trainer.csv\", index=False, quoting=3)\nlogging.info('result saved!')","metadata":{"execution":{"iopub.status.busy":"2023-01-13T11:51:45.385068Z","iopub.execute_input":"2023-01-13T11:51:45.385460Z","iopub.status.idle":"2023-01-13T11:51:45.419084Z","shell.execute_reply.started":"2023-01-13T11:51:45.385424Z","shell.execute_reply":"2023-01-13T11:51:45.418233Z"},"trusted":true},"execution_count":12,"outputs":[]}]}